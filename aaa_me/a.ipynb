{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXACT09已按作者所述处理(参考github)\n",
    "本机(fudan104)上的环境为airway_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "未解决的问题: out_channel为什么是两维？准确率怎么计算\n",
    "如果我调整了batch_size，是否需要调整其他超参数？\n",
    "low和high是怎么样起作用的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def set_para(self, file_format='.npy', crop_size=32, windowMin=-1000, windowMax=150,\\\n",
    "        need_tensor_output=True, need_transform=True):\n",
    "        self.para[\"file_format\"] = file_format#'.npy'\n",
    "        self.para[\"crop_size\"] = crop_size#32\n",
    "        self.para[\"windowMin\"] = windowMin#-1000\n",
    "        self.para[\"windowMax\"] = windowMax#150\n",
    "        self.para[\"need_tensor_output\"] = need_tensor_output#True\n",
    "        self.para[\"need_transform\"] = need_transform#True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler_of_airways_of_low_generation = RandomSampler(train_dataset,num_samples = min(num_samples_of_each_epoch, len(train_dataset)), replacement = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "已知back_pix_per是背景像素所占比例\n",
    "groundtruth_foreground=batch['label'].float().to(device)\n",
    "    roundtruth_background=1-groundtruth_foreground\n",
    "weights = (torch.exp(back_pix_per)/(torch.exp(fore_pix_per)+torch.exp(back_pix_per))*torch.eq(groundtruth_foreground,1).float()+\\\n",
    "            torch.exp(fore_pix_per)/(torch.exp(fore_pix_per)+torch.exp(back_pix_per))*torch.eq(groundtruth_foreground,0).float()).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=SegAirwayModel(in_channels=1, out_channels=2)\n",
    "#/home/cs22-wangc/now/NaviAirway/func/model_arch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1],\n",
       "          [ 2],\n",
       "          [ 3],\n",
       "          [ 4],\n",
       "          [ 5],\n",
       "          [ 6]],\n",
       " \n",
       "         [[ 7],\n",
       "          [ 8],\n",
       "          [ 9],\n",
       "          [10],\n",
       "          [11],\n",
       "          [12]],\n",
       " \n",
       "         [[13],\n",
       "          [14],\n",
       "          [15],\n",
       "          [16],\n",
       "          [17],\n",
       "          [18]]]),\n",
       " tensor([[[ 1,  1],\n",
       "          [ 2,  2],\n",
       "          [ 3,  3],\n",
       "          [ 4,  4],\n",
       "          [ 5,  5],\n",
       "          [ 6,  6]],\n",
       " \n",
       "         [[ 7,  7],\n",
       "          [ 8,  8],\n",
       "          [ 9,  9],\n",
       "          [10, 10],\n",
       "          [11, 11],\n",
       "          [12, 12]],\n",
       " \n",
       "         [[13, 13],\n",
       "          [14, 14],\n",
       "          [15, 15],\n",
       "          [16, 16],\n",
       "          [17, 17],\n",
       "          [18, 18]]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "list1=[]\n",
    "for i in range(1,19):\n",
    "    list1.append(i)\n",
    "a=torch.tensor(list1)\n",
    "a=a.view(3,6,1)\n",
    "c=torch.rand(3,6,2)\n",
    "b=a.expand_as(c)\n",
    "a,b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转置卷积nn.ConvTranspose2d\n",
    "# 该函数是用来进行转置卷积的，它主要做了这几件事：\n",
    "#     首先，对输入的feature map进行padding操作，得到新的feature map；\n",
    "#     然后，随机初始化一定尺寸的卷积核；\n",
    "#     最后，用随机初始化的一定尺寸的卷积核在新的feature map上进行卷积操作。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 32, 32]) torch.Size([1, 32, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 输入特征图大小\n",
    "input_channels = 64\n",
    "input_height = 32\n",
    "input_width = 32\n",
    "\n",
    "# 输出特征图大小\n",
    "output_channels = 32\n",
    "output_height = 64\n",
    "output_width = 64\n",
    "\n",
    "# 转置卷积层\n",
    "conv_transpose = torch.nn.ConvTranspose2d(\n",
    "    in_channels=input_channels,\n",
    "    out_channels=output_channels,\n",
    "    kernel_size=3,\n",
    "    stride=2,\n",
    "    padding=1,\n",
    "    output_padding=1\n",
    ")\n",
    "\n",
    "# 随机生成输入特征图\n",
    "input_tensor = torch.randn(1, input_channels, input_height, input_width)\n",
    "\n",
    "# 执行转置卷积操作\n",
    "output_tensor = conv_transpose(input_tensor)\n",
    "\n",
    "# 输出特征图大小\n",
    "print(input_tensor.size(),output_tensor.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在 Python 中，@staticmethod 是一个装饰器，用于将函数转化为静态方法。静态方法是类中的一个特殊方法，它可以在不需要创建类实例的情况下被调用。\n",
    "# 与类方法和实例方法不同，静态方法没有默认的第一个参数，即 self 或 cls。\n",
    "# 因此，它可以被类或类的实例直接调用，而不需要传递实例或类作为第一个参数。\n",
    "\n",
    "# 静态方法在许多情况下非常有用。\n",
    "# 例如，当一个方法与类的状态无关时，我们可以将其定义为静态方法。\n",
    "# 这样做不仅可以提高代码的可读性，还可以避免在不需要时创建类实例。\n",
    "\n",
    "# 下面是一个使用 @staticmethod 装饰器定义静态方法的示例代码：\n",
    "\n",
    "class MyClass:\n",
    "    @staticmethod\n",
    "    def my_static_method(arg1, arg2):\n",
    "        # do something with arg1 and arg2\n",
    "        return result\n",
    "    \n",
    "# 在上面的代码中，我们使用 @staticmethod 装饰器将 my_static_method 方法定义为 MyClass 类的静态方法。\n",
    "# 该方法可以直接通过类名或类的实例进行调用，而不需要传递实例或类作为第一个参数。\n",
    "\n",
    "# 在 Python 3 中，静态方法也可以使用内置的 staticmethod() 函数来定义，如下所示：\n",
    "class MyClass:\n",
    "    @staticmethod\n",
    "    def my_static_method(arg1, arg2):\n",
    "        # do something with arg1 and arg2\n",
    "        return result\n",
    "\n",
    "MyClass.my_static_method(arg1, arg2)  # 调用静态方法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面是一个使用 @staticmethod 装饰器定义排序算法的示例代码：\n",
    "class Sorter:\n",
    "    @staticmethod\n",
    "    def sort_numbers(numbers):\n",
    "        \"\"\"\n",
    "        对一组数进行排序，返回排序后的结果。\n",
    "        :param numbers: 要排序的数，以列表的形式传入。\n",
    "        :return: 排序后的结果，以列表的形式返回。\n",
    "        \"\"\"\n",
    "        return sorted(numbers)\n",
    "\n",
    "# 使用静态方法进行排序\n",
    "numbers = [3, 1, 4, 2, 5]\n",
    "sorted_numbers = Sorter.sort_numbers(numbers)\n",
    "print(sorted_numbers)  # 输出 [1, 2, 3, 4, 5]\n",
    "# 在上面的代码中，我们使用 @staticmethod 装饰器将 sort_numbers 方法定义为 Sorter 类的静态方法。\n",
    "# 该方法接收一个列表作为参数，并返回排序后的结果。\n",
    "\n",
    "# 我们可以直接使用 Sorter.sort_numbers() 方法对一组数进行排序，而不需要创建 Sorter 类的实例。\n",
    "# 这样做不仅可以提高代码的可读性，还可以避免在不需要时创建类实例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 静态方法有两个好处:1.不需要定义实例就可以使用函数。 2.在用def创建函数时，不需要默认的self参数。 \n",
    "# 我说的够完善吗？你还有补充吗？\n",
    "\n",
    "\n",
    "# 是的，你说的很对。除了你提到的两个好处之外，静态方法还有以下一些特点和用途：\n",
    "\n",
    "# 静态方法是属于类的方法，而不是实例的方法。\n",
    "# 因此，它们不能访问实例变量或实例方法。静态方法通常用于执行与类或实例无关的任务，如通用的辅助函数或工具函数。\n",
    "\n",
    "# 静态方法可以使用类或实例进行调用，但它们不会自动获取类或实例的属性。因此，静态方法通常需要传递所有必要的参数。\n",
    "\n",
    "# 静态方法不会改变类或实例的状态。因此，它们可以在不改变对象状态的情况下执行某些操作。\n",
    "# 这使得静态方法非常适合在不需要修改对象状态的情况下执行某些操作。\n",
    "\n",
    "# 静态方法的另一个用途是封装和组织代码。将相关的函数封装成静态方法可以使代码更具可读性和可维护性，并帮助避免命名冲突和重复代码。\n",
    "\n",
    "# 综上所述，静态方法是一种非常有用的编程工具，可以提高代码的可读性、可维护性和可重用性，并帮助在不需要创建类实例时执行某些操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available.\n",
      "There are 4 CUDA devices available.\n",
      "Device 0: NVIDIA GeForce RTX 2080 Ti\n",
      "Device 1: NVIDIA GeForce RTX 2080 Ti\n",
      "Device 2: NVIDIA GeForce RTX 2080 Ti\n",
      "Device 3: NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available.\")\n",
    "    device_count = torch.cuda.device_count()\n",
    "    print(f\"There are {device_count} CUDA devices available.\")\n",
    "    for i in range(device_count):\n",
    "        device_name = torch.cuda.get_device_name(i)\n",
    "        print(f\"Device {i}: {device_name}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: <torch.cuda.device object at 0x7f3d9659c090>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if str(device) == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Select an unused CUDA device\n",
    "    device = torch.cuda.device(None)\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "# Your code here...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a={'a':2,'b':8}\n",
    "b={1:2,2:8,3:9}\n",
    "test={1:a,2:b}\n",
    "# for key,value in enumerate(test):\n",
    "#     print(key,value)\n",
    "m=list(a)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in test.items():\n",
    "    print(key,value)\n",
    "# 在第一个例子中，使用的是 enumerate() 函数，它返回的是一个包含 (index, key) 元组的迭代器，\n",
    "# 其中 index 是迭代器的下标，key 是字典中的键。因此，key 变量包含的是字典的键而不是字典本身的值。这种方式通常用于对字典键进行迭代。\n",
    "\n",
    "# 而在第二个例子中，使用的是 items() 方法，它返回的是一个包含 (key, value) 元组的迭代器，\n",
    "# 其中 key 是字典中的键，value 是字典中的值。因此，value 变量包含的是字典中的值，而不是字典中的键。这种方式通常用于对字典的键值对进行迭代。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytracking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e2eff005cba963436423ee35268d3875b7076a7dfef8bee54735675e9a1cba2f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
