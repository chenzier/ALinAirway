{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangc/anaconda3/envs/test1/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from torch import from_numpy as from_numpy\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")  # 将上一层目录添加到模块搜索路径中\n",
    "\n",
    "from active_utils.dataset_process_tools import DatasetInfo\n",
    "from active_utils.embedding_tools import (\n",
    "    get_embeddings,\n",
    "    load_partial_embeddings2,\n",
    "    load_partial_embeddings3,\n",
    ")\n",
    "from active_utils.visualize_tools import (\n",
    "    visualize_and_return_indices,\n",
    "    show_all_2d_img_with_labels,\n",
    ")\n",
    "from active_utils.cluster_tools import kmeans\n",
    "from active_utils.file_tools import load_chunks, print_memory_usage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start | Memory Usage: 399.86 MB\n",
      "Processed 1/1976 files | Memory Usage: 404.72 MB\n",
      "Processed 1001/1976 files | Memory Usage: 406.38 MB\n",
      "Processed 1976/1976 files | Memory Usage: 407.83 MB\n",
      "exact stacked is done | Memory Usage: 32024.34 MB\n",
      "Processed 1/5150 files | Memory Usage: 32031.18 MB\n",
      "Processed 1001/5150 files | Memory Usage: 32031.90 MB\n",
      "Processed 2001/5150 files | Memory Usage: 32032.73 MB\n",
      "Processed 3001/5150 files | Memory Usage: 32034.05 MB\n",
      "Processed 4001/5150 files | Memory Usage: 32035.70 MB\n",
      "Processed 5001/5150 files | Memory Usage: 32037.04 MB\n",
      "Processed 5150/5150 files | Memory Usage: 32037.36 MB\n",
      "lidc stacked is done | Memory Usage: 114437.90 MB\n",
      "done | Memory Usage: 228429.69 MB\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 50\n",
    "device2 = torch.device(\"cuda:6\" if torch.cuda.is_available() else \"cpu\")\n",
    "test_names = [\n",
    "    \"LIDC_IDRI_0066\",\n",
    "    \"LIDC_IDRI_0328\",\n",
    "    \"LIDC_IDRI_0376\",\n",
    "    \"LIDC_IDRI_0441\",\n",
    "    \"LIDC_IDRI_0744\",\n",
    "    \"LIDC_IDRI_1004\",\n",
    "    \"EXACT09_CASE13\",\n",
    "    \"EXACT09_CASE08\",\n",
    "    \"EXACT09_CASE01\",\n",
    "    \"EXACT09_CASE05\",\n",
    "]\n",
    "\n",
    "LidcInfo = DatasetInfo(\"/mnt/wangc/LIDC/Precrop_dataset_for_LIDC-IDRI_128\")\n",
    "LidcInfo.get_case_names(\"/mnt/wangc/LIDC\", \"lidc\")\n",
    "\n",
    "Exact09Info = DatasetInfo(\"/mnt/wangc/EXACT09/Precrop_dataset_for_EXACT09_128\")\n",
    "Exact09Info.get_case_names(\"/mnt/wangc/EXACT09/EXACT09_3D\", \"exact09\")\n",
    "\n",
    "crop_size = [\"128\", \"256\"]\n",
    "file_insert = crop_size[0]\n",
    "exact_embedding_folder_path = (\n",
    "    f\"/data/wangc/al_data/test1123/embedding/exact09_{file_insert}_op_embeddings_folder\"\n",
    ")\n",
    "lidc_embedding_folder_path = (\n",
    "    f\"/data/wangc/al_data/test1123/embedding/lidc_{file_insert}_op_embeddings_folder\"\n",
    ")\n",
    "\n",
    "\n",
    "print_memory_usage(\"Start\")\n",
    "\n",
    "# exact_embeddings_dict = load_chunks(exact_embedding_folder_path)\n",
    "# print_memory_usage(\"load1 exact is done\")\n",
    "# lidc_embeddings_dict = load_chunks(lidc_embedding_folder_path)\n",
    "# print_memory_usage(\"load1 lidc is done\")\n",
    "# print_memory_usage(\"load1 is done\")\n",
    "\n",
    "exact_lidc_concatenated_array, merged_list = load_partial_embeddings3(\n",
    "    exact_embedding_folder_path,\n",
    "    lidc_embedding_folder_path,\n",
    "    train_names=None,\n",
    "    test_names=test_names,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load is done | Memory Usage: 114413.55 MB\n",
      "kmeans start | Memory Usage: 114413.61 MB\n",
      "running k-means on cpu..\n",
      "torch.Size([2, 4194304])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[running kmeans]: 0it [00:00, ?it/s]"
     ]
    }
   ],
   "source": [
    "print_memory_usage(\"load is done\")\n",
    "data_shape = exact_lidc_concatenated_array.shape\n",
    "\n",
    "X_t = exact_lidc_concatenated_array.reshape(data_shape[0], -1)\n",
    "# 需要把数据放到GPU上\n",
    "X_t = from_numpy(X_t).float()\n",
    "X_t_expanded = X_t.unsqueeze(1)\n",
    "# X_t_expanded = X_t_expanded.to(device2)\n",
    "N = X_t.shape[0]\n",
    "\n",
    "print_memory_usage(\"kmeans start\")\n",
    "num_cluster = 2\n",
    "cluster_labels, cluster_centers = kmeans(\n",
    "    X=X_t, num_clusters=num_cluster, init=None, distance=\"euclidean\"\n",
    ")\n",
    "cluster_centers_expanded = cluster_centers.unsqueeze(0)\n",
    "cluster_centers_expanded = cluster_centers_expanded.to(device2)\n",
    "print_memory_usage(\"kmeans is done\")\n",
    "\n",
    "uncertainy_dict = {}\n",
    "for i in range(0, N, batch_size):\n",
    "    # Select a batch of data\n",
    "    X_batch = X_t_expanded[i : i + batch_size]\n",
    "\n",
    "    # Calculate distances for the batch\n",
    "    distances_batch = torch.sqrt(\n",
    "        torch.sum((X_batch - cluster_centers_expanded) ** 2, dim=2)\n",
    "    )\n",
    "\n",
    "    # Calculate uncertainty for the batch\n",
    "    uncertainy_batch = torch.abs(distances_batch[:, 0] - distances_batch[:, 1])\n",
    "\n",
    "    # Update uncertainy_dict with batch results\n",
    "    for j in range(batch_size):\n",
    "        index = i + j\n",
    "        if index < N:\n",
    "            uncertainy_dict[merged_list[index]] = uncertainy_batch[j].cpu().numpy()\n",
    "\n",
    "\n",
    "res_path = \"/data/wangc/al_data/test1123/uncertainy/kmeans_no_init_cluster_2_test_1202_1909.pkl\"\n",
    "# 确保文件夹存在，如果不存在则创建它\n",
    "os.makedirs(os.path.dirname(res_path), exist_ok=True)\n",
    "\n",
    "print_memory_usage(\"saving\")\n",
    "# 保存到文件\n",
    "with open(res_path, \"wb\") as file:\n",
    "    data_to_save = {\"uncertainy_dict\": uncertainy_dict}\n",
    "    pickle.dump(data_to_save, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load is done | Memory Usage: 117595.80 MB\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 111.34 GiB (GPU 6; 23.68 GiB total capacity; 0 bytes already allocated; 22.19 GiB free; 0 bytes reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m X_t \u001b[38;5;241m=\u001b[39m exact_lidc_concatenated_array\u001b[38;5;241m.\u001b[39mreshape(data_shape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 需要把数据放到GPU上\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m X_t \u001b[38;5;241m=\u001b[39m \u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_t\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice2\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 111.34 GiB (GPU 6; 23.68 GiB total capacity; 0 bytes already allocated; 22.19 GiB free; 0 bytes reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "print_memory_usage(\"load is done\")\n",
    "data_shape = exact_lidc_concatenated_array.shape\n",
    "\n",
    "X_t = exact_lidc_concatenated_array.reshape(data_shape[0], -1)\n",
    "# 需要把数据放到GPU上\n",
    "X_t = from_numpy(X_t).float().to(device2)\n",
    "X_t_expanded = X_t.unsqueeze(1)\n",
    "X_t_expanded = X_t_expanded.to(device2)\n",
    "N = X_t.shape[0]\n",
    "\n",
    "print_memory_usage(\"kmeans start\")\n",
    "num_cluster = 2\n",
    "cluster_labels, cluster_centers = kmeans(\n",
    "    X=X_t, num_clusters=num_cluster, init=None, distance=\"euclidean\", device=device2\n",
    ")\n",
    "cluster_centers_expanded = cluster_centers.unsqueeze(0)\n",
    "cluster_centers_expanded = cluster_centers_expanded.to(device2)\n",
    "print_memory_usage(\"kmeans is done\")\n",
    "\n",
    "uncertainy_dict = {}\n",
    "for i in range(0, N, batch_size):\n",
    "    # Select a batch of data\n",
    "    X_batch = X_t_expanded[i : i + batch_size]\n",
    "\n",
    "    # Calculate distances for the batch\n",
    "    distances_batch = torch.sqrt(\n",
    "        torch.sum((X_batch - cluster_centers_expanded) ** 2, dim=2)\n",
    "    )\n",
    "\n",
    "    # Calculate uncertainty for the batch\n",
    "    uncertainy_batch = torch.abs(distances_batch[:, 0] - distances_batch[:, 1])\n",
    "\n",
    "    # Update uncertainy_dict with batch results\n",
    "    for j in range(batch_size):\n",
    "        index = i + j\n",
    "        if index < N:\n",
    "            uncertainy_dict[merged_list[index]] = uncertainy_batch[j].cpu().numpy()\n",
    "\n",
    "\n",
    "res_path = \"/data/wangc/al_data/test1123/uncertainy/kmeans_no_init_cluster_2_test_1129_2056.pkl\"\n",
    "# 确保文件夹存在，如果不存在则创建它\n",
    "os.makedirs(os.path.dirname(res_path), exist_ok=True)\n",
    "\n",
    "print_memory_usage(\"saving\")\n",
    "# 保存到文件\n",
    "with open(res_path, \"wb\") as file:\n",
    "    data_to_save = {\"uncertainy_dict\": uncertainy_dict}\n",
    "    pickle.dump(data_to_save, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_t_expanded = X_t.unsqueeze(1)\n",
    "X_t_expanded = X_t_expanded.to(device2)\n",
    "N = X_t.shape[0]\n",
    "\n",
    "print_memory_usage(\"kmeans start\")\n",
    "num_cluster = 2\n",
    "cluster_labels, cluster_centers = kmeans(\n",
    "    X=X_t, num_clusters=num_cluster, init=None, distance=\"euclidean\", device=device2\n",
    ")\n",
    "cluster_centers_expanded = cluster_centers.unsqueeze(0)\n",
    "cluster_centers_expanded = cluster_centers_expanded.to(device2)\n",
    "print_memory_usage(\"kmeans is done\")\n",
    "\n",
    "uncertainy_dict = {}\n",
    "for i in range(0, N, batch_size):\n",
    "    # Select a batch of data\n",
    "    X_batch = X_t_expanded[i : i + batch_size]\n",
    "\n",
    "    # Calculate distances for the batch\n",
    "    distances_batch = torch.sqrt(\n",
    "        torch.sum((X_batch - cluster_centers_expanded) ** 2, dim=2)\n",
    "    )\n",
    "\n",
    "    # Calculate uncertainty for the batch\n",
    "    uncertainy_batch = torch.abs(distances_batch[:, 0] - distances_batch[:, 1])\n",
    "\n",
    "    # Update uncertainy_dict with batch results\n",
    "    for j in range(batch_size):\n",
    "        index = i + j\n",
    "        if index < N:\n",
    "            uncertainy_dict[merged_list[index]] = uncertainy_batch[j].cpu().numpy()\n",
    "\n",
    "\n",
    "res_path = \"/data/wangc/al_data/test1123/uncertainy/kmeans_no_init_cluster_2_test_1129_2056.pkl\"\n",
    "# 确保文件夹存在，如果不存在则创建它\n",
    "os.makedirs(os.path.dirname(res_path), exist_ok=True)\n",
    "\n",
    "print_memory_usage(\"saving\")\n",
    "# 保存到文件\n",
    "with open(res_path, \"wb\") as file:\n",
    "    data_to_save = {\"uncertainy_dict\": uncertainy_dict}\n",
    "    pickle.dump(data_to_save, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_t_expanded = X_t.unsqueeze(1)\n",
    "X_t_expanded = X_t_expanded.to(device2)\n",
    "N = X_t.shape[0]\n",
    "\n",
    "print_memory_usage(\"kmeans start\")\n",
    "num_cluster = 2\n",
    "cluster_labels, cluster_centers = kmeans(\n",
    "    X=X_t, num_clusters=num_cluster, init=None, distance=\"euclidean\", device=device2\n",
    ")\n",
    "cluster_centers_expanded = cluster_centers.unsqueeze(0)\n",
    "cluster_centers_expanded = cluster_centers_expanded.to(device2)\n",
    "print_memory_usage(\"kmeans is done\")\n",
    "\n",
    "uncertainy_dict = {}\n",
    "for i in range(0, N, batch_size):\n",
    "    # Select a batch of data\n",
    "    X_batch = X_t_expanded[i : i + batch_size]\n",
    "\n",
    "    # Calculate distances for the batch\n",
    "    distances_batch = torch.sqrt(\n",
    "        torch.sum((X_batch - cluster_centers_expanded) ** 2, dim=2)\n",
    "    )\n",
    "\n",
    "    # Calculate uncertainty for the batch\n",
    "    uncertainy_batch = torch.abs(distances_batch[:, 0] - distances_batch[:, 1])\n",
    "\n",
    "    # Update uncertainy_dict with batch results\n",
    "    for j in range(batch_size):\n",
    "        index = i + j\n",
    "        if index < N:\n",
    "            uncertainy_dict[merged_list[index]] = uncertainy_batch[j].cpu().numpy()\n",
    "\n",
    "\n",
    "res_path = \"/data/wangc/al_data/test1123/uncertainy/kmeans_no_init_cluster_2_test_1129_2056.pkl\"\n",
    "# 确保文件夹存在，如果不存在则创建它\n",
    "os.makedirs(os.path.dirname(res_path), exist_ok=True)\n",
    "\n",
    "print_memory_usage(\"saving\")\n",
    "# 保存到文件\n",
    "with open(res_path, \"wb\") as file:\n",
    "    data_to_save = {\"uncertainy_dict\": uncertainy_dict}\n",
    "    pickle.dump(data_to_save, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
